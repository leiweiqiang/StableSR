#!/usr/bin/env python3
"""
Edge Mapæ€§èƒ½æµ‹è¯•è„šæœ¬
ä¸“é—¨ç”¨äºæµ‹è¯•edge mapå¤„ç†çš„æ€§èƒ½æŒ‡æ ‡

ä½¿ç”¨æ–¹æ³•:
python test_edge_map_performance.py
python test_edge_map_performance.py --device cuda --batch_sizes 1,2,4,8
"""

import os
import sys
import argparse
import time
import torch
import numpy as np
import cv2
import psutil
import gc
from pathlib import Path
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class EdgeMapPerformanceTester:
    """Edge Mapæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, device=None, output_dir="performance_test_results"):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # ç³»ç»Ÿä¿¡æ¯
        self.system_info = self.get_system_info()
        
    def get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            "device": str(self.device),
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total / (1024**3),  # GB
            "memory_available": psutil.virtual_memory().available / (1024**3),  # GB
            "python_version": sys.version,
        }
        
        if torch.cuda.is_available():
            info.update({
                "cuda_available": True,
                "cuda_version": torch.version.cuda,
                "cuda_device_count": torch.cuda.device_count(),
                "cuda_device_name": torch.cuda.get_device_name(0),
                "cuda_memory_total": torch.cuda.get_device_properties(0).total_memory / (1024**3),  # GB
            })
        else:
            info["cuda_available"] = False
        
        return info
    
    def create_test_edge_map(self, size=(512, 512)):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„edge map"""
        h, w = size
        img = np.zeros((h, w, 3), dtype=np.uint8)
        
        # åˆ›å»ºå¤æ‚çš„æµ‹è¯•å›¾æ¡ˆ
        cv2.rectangle(img, (50, 50), (200, 200), (255, 255, 255), -1)
        cv2.circle(img, (350, 350), 80, (128, 128, 128), -1)
        cv2.line(img, (100, 300), (400, 100), (200, 200, 200), 3)
        cv2.ellipse(img, (250, 150), (60, 40), 45, 0, 360, (180, 180, 180), -1)
        
        # æ·»åŠ çº¹ç†
        for i in range(0, h, 50):
            for j in range(0, w, 50):
                if (i + j) % 100 == 0:
                    cv2.rectangle(img, (j, i), (j+20, i+20), (100, 100, 100), -1)
        
        # è½¬æ¢ä¸ºedge map
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)
        edges = cv2.Canny(blurred, threshold1=100, threshold2=200)
        edges_3ch = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        
        return edges_3ch
    
    def benchmark_edge_generation(self, sizes=None, iterations=10):
        """åŸºå‡†æµ‹è¯•edge mapç”Ÿæˆ"""
        if sizes is None:
            sizes = [(256, 256), (512, 512), (1024, 1024)]
        
        print("\n" + "="*50)
        print("Edge Mapç”Ÿæˆæ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        results = []
        
        for size in sizes:
            print(f"\næµ‹è¯•å°ºå¯¸: {size}")
            
            # åˆ›å»ºæµ‹è¯•å›¾åƒ
            test_image = self.create_test_edge_map(size)
            
            # æµ‹è¯•ä¸åŒæ–¹æ³•
            methods = ["canny", "sobel", "laplacian"]
            
            for method in methods:
                print(f"  æµ‹è¯•æ–¹æ³•: {method}")
                
                times = []
                memory_usage = []
                
                for i in range(iterations):
                    # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
                    start_time = time.time()
                    start_memory = psutil.Process().memory_info().rss / (1024**2)  # MB
                    
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                        start_gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
                    
                    # æ‰§è¡Œedgeæ£€æµ‹
                    if method == "canny":
                        gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)
                        blurred = cv2.GaussianBlur(gray, (5, 5), 1.4)
                        edges = cv2.Canny(blurred, 100, 200)
                    elif method == "sobel":
                        gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)
                        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                        edges = np.sqrt(sobel_x**2 + sobel_y**2)
                        edges = np.uint8(edges / edges.max() * 255)
                    elif method == "laplacian":
                        gray = cv2.cvtColor(test_image, cv2.COLOR_BGR2GRAY)
                        edges = cv2.Laplacian(gray, cv2.CV_64F)
                        edges = np.uint8(np.absolute(edges))
                    
                    # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                        end_gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
                    
                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / (1024**2)  # MB
                    
                    # è®¡ç®—æ—¶é—´å’Œå†…å­˜ä½¿ç”¨
                    elapsed_time = end_time - start_time
                    memory_delta = end_memory - start_memory
                    
                    times.append(elapsed_time)
                    memory_usage.append(memory_delta)
                    
                    # æ¸…ç†å†…å­˜
                    del edges
                    gc.collect()
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                avg_time = np.mean(times)
                std_time = np.std(times)
                min_time = np.min(times)
                max_time = np.max(times)
                avg_memory = np.mean(memory_usage)
                
                result = {
                    "size": size,
                    "method": method,
                    "iterations": iterations,
                    "avg_time": avg_time,
                    "std_time": std_time,
                    "min_time": min_time,
                    "max_time": max_time,
                    "avg_memory": avg_memory,
                    "pixels_per_second": (size[0] * size[1]) / avg_time
                }
                
                results.append(result)
                
                print(f"    å¹³å‡æ—¶é—´: {avg_time:.4f}s Â± {std_time:.4f}s")
                print(f"    æ—¶é—´èŒƒå›´: [{min_time:.4f}s, {max_time:.4f}s]")
                print(f"    å¹³å‡å†…å­˜: {avg_memory:.2f}MB")
                print(f"    å¤„ç†é€Ÿåº¦: {result['pixels_per_second']:.0f} pixels/s")
        
        return results
    
    def benchmark_edge_processor(self, batch_sizes=None, sizes=None, iterations=5):
        """åŸºå‡†æµ‹è¯•edgeå¤„ç†å™¨"""
        if batch_sizes is None:
            batch_sizes = [1, 2, 4, 8]
        if sizes is None:
            sizes = [(256, 256), (512, 512)]
        
        print("\n" + "="*50)
        print("Edgeå¤„ç†å™¨æ€§èƒ½æµ‹è¯•")
        print("="*50)
        
        try:
            from ldm.modules.diffusionmodules.edge_processor import EdgeMapProcessor
        except ImportError:
            print("âš ï¸  Edgeå¤„ç†å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
            return []
        
        results = []
        
        # åˆ›å»ºå¤„ç†å™¨
        processor = EdgeMapProcessor(
            input_channels=3,
            output_channels=4,
            target_size=64,
            use_checkpoint=False
        ).to(self.device)
        
        for size in sizes:
            for batch_size in batch_sizes:
                print(f"\næµ‹è¯•é…ç½®: å°ºå¯¸={size}, batch_size={batch_size}")
                
                # å‡†å¤‡æµ‹è¯•æ•°æ®
                edge_maps = []
                for _ in range(batch_size):
                    edge_map_np = self.create_test_edge_map(size)
                    edge_tensor = torch.from_numpy(edge_map_np).float().permute(2, 0, 1) / 255.0
                    edge_maps.append(edge_tensor)
                
                edge_batch = torch.stack(edge_maps).to(self.device)
                
                # é¢„çƒ­
                with torch.no_grad():
                    _ = processor(edge_batch)
                
                # æ€§èƒ½æµ‹è¯•
                times = []
                memory_usage = []
                
                for i in range(iterations):
                    # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
                    start_time = time.time()
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                        start_gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
                    
                    # æ‰§è¡Œå¤„ç†
                    with torch.no_grad():
                        output = processor(edge_batch)
                    
                    # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                        end_gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
                    
                    end_time = time.time()
                    
                    # è®¡ç®—æ—¶é—´å’Œå†…å­˜ä½¿ç”¨
                    elapsed_time = end_time - start_time
                    gpu_memory_delta = (end_gpu_memory - start_gpu_memory) if torch.cuda.is_available() else 0
                    
                    times.append(elapsed_time)
                    memory_usage.append(gpu_memory_delta)
                    
                    # æ¸…ç†å†…å­˜
                    del output
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                avg_time = np.mean(times)
                std_time = np.std(times)
                min_time = np.min(times)
                max_time = np.max(times)
                avg_memory = np.mean(memory_usage)
                
                result = {
                    "size": size,
                    "batch_size": batch_size,
                    "iterations": iterations,
                    "avg_time": avg_time,
                    "std_time": std_time,
                    "min_time": min_time,
                    "max_time": max_time,
                    "avg_memory": avg_memory,
                    "throughput": batch_size / avg_time,  # samples per second
                    "pixels_per_second": (batch_size * size[0] * size[1]) / avg_time
                }
                
                results.append(result)
                
                print(f"  å¹³å‡æ—¶é—´: {avg_time:.4f}s Â± {std_time:.4f}s")
                print(f"  æ—¶é—´èŒƒå›´: [{min_time:.4f}s, {max_time:.4f}s]")
                print(f"  å¹³å‡GPUå†…å­˜: {avg_memory:.2f}MB")
                print(f"  ååé‡: {result['throughput']:.2f} samples/s")
                print(f"  å¤„ç†é€Ÿåº¦: {result['pixels_per_second']:.0f} pixels/s")
                
                # æ¸…ç†å†…å­˜
                del edge_batch
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return results
    
    def benchmark_memory_usage(self, max_size=(2048, 2048)):
        """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
        print("\n" + "="*50)
        print("å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•")
        print("="*50)
        
        results = []
        sizes = [(256, 256), (512, 512), (1024, 1024), (1536, 1536), (2048, 2048)]
        
        for size in sizes:
            if size[0] > max_size[0] or size[1] > max_size[1]:
                continue
                
            print(f"\næµ‹è¯•å°ºå¯¸: {size}")
            
            try:
                # æµ‹è¯•edge mapç”Ÿæˆå†…å­˜ä½¿ç”¨
                start_memory = psutil.Process().memory_info().rss / (1024**2)  # MB
                
                edge_map = self.create_test_edge_map(size)
                
                end_memory = psutil.Process().memory_info().rss / (1024**2)  # MB
                memory_usage = end_memory - start_memory
                
                # æµ‹è¯•tensorè½¬æ¢å†…å­˜ä½¿ç”¨
                edge_tensor = torch.from_numpy(edge_map).float().permute(2, 0, 1).unsqueeze(0) / 255.0
                edge_tensor = edge_tensor.to(self.device)
                
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / (1024**2)  # MB
                else:
                    gpu_memory = 0
                
                result = {
                    "size": size,
                    "pixels": size[0] * size[1],
                    "cpu_memory": memory_usage,
                    "gpu_memory": gpu_memory,
                    "memory_per_pixel": memory_usage / (size[0] * size[1]) * 1024  # KB per pixel
                }
                
                results.append(result)
                
                print(f"  CPUå†…å­˜ä½¿ç”¨: {memory_usage:.2f}MB")
                print(f"  GPUå†…å­˜ä½¿ç”¨: {gpu_memory:.2f}MB")
                print(f"  æ¯åƒç´ å†…å­˜: {result['memory_per_pixel']:.4f}KB")
                
                # æ¸…ç†å†…å­˜
                del edge_map, edge_tensor
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                gc.collect()
                
            except Exception as e:
                print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        
        return results
    
    def save_results(self, edge_gen_results, processor_results, memory_results):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        # ä¿å­˜JSONç»“æœ
        all_results = {
            "system_info": self.system_info,
            "timestamp": datetime.now().isoformat(),
            "edge_generation": edge_gen_results,
            "edge_processor": processor_results,
            "memory_usage": memory_results
        }
        
        json_path = self.output_dir / "performance_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        report_path = self.output_dir / "performance_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("Edge Mapæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è®¾å¤‡: {self.system_info['device']}\n")
            f.write(f"CPUæ ¸å¿ƒæ•°: {self.system_info['cpu_count']}\n")
            f.write(f"æ€»å†…å­˜: {self.system_info['memory_total']:.1f}GB\n")
            
            if self.system_info['cuda_available']:
                f.write(f"CUDAè®¾å¤‡: {self.system_info['cuda_device_name']}\n")
                f.write(f"CUDAå†…å­˜: {self.system_info['cuda_memory_total']:.1f}GB\n")
            
            f.write("\n" + "=" * 50 + "\n")
            f.write("Edge Mapç”Ÿæˆæ€§èƒ½\n")
            f.write("=" * 50 + "\n")
            
            for result in edge_gen_results:
                f.write(f"å°ºå¯¸: {result['size']}, æ–¹æ³•: {result['method']}\n")
                f.write(f"  å¹³å‡æ—¶é—´: {result['avg_time']:.4f}s Â± {result['std_time']:.4f}s\n")
                f.write(f"  å¤„ç†é€Ÿåº¦: {result['pixels_per_second']:.0f} pixels/s\n")
                f.write(f"  å†…å­˜ä½¿ç”¨: {result['avg_memory']:.2f}MB\n\n")
            
            f.write("\n" + "=" * 50 + "\n")
            f.write("Edgeå¤„ç†å™¨æ€§èƒ½\n")
            f.write("=" * 50 + "\n")
            
            for result in processor_results:
                f.write(f"å°ºå¯¸: {result['size']}, Batch: {result['batch_size']}\n")
                f.write(f"  å¹³å‡æ—¶é—´: {result['avg_time']:.4f}s Â± {result['std_time']:.4f}s\n")
                f.write(f"  ååé‡: {result['throughput']:.2f} samples/s\n")
                f.write(f"  å¤„ç†é€Ÿåº¦: {result['pixels_per_second']:.0f} pixels/s\n")
                f.write(f"  GPUå†…å­˜: {result['avg_memory']:.2f}MB\n\n")
            
            f.write("\n" + "=" * 50 + "\n")
            f.write("å†…å­˜ä½¿ç”¨åˆ†æ\n")
            f.write("=" * 50 + "\n")
            
            for result in memory_results:
                f.write(f"å°ºå¯¸: {result['size']}\n")
                f.write(f"  CPUå†…å­˜: {result['cpu_memory']:.2f}MB\n")
                f.write(f"  GPUå†…å­˜: {result['gpu_memory']:.2f}MB\n")
                f.write(f"  æ¯åƒç´ å†…å­˜: {result['memory_per_pixel']:.4f}KB\n\n")
        
        print(f"\nç»“æœå·²ä¿å­˜:")
        print(f"  JSON: {json_path}")
        print(f"  æŠ¥å‘Š: {report_path}")
    
    def run_all_tests(self, batch_sizes=None, max_size=(2048, 2048)):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("å¼€å§‹Edge Mapæ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # Edge mapç”Ÿæˆæ€§èƒ½æµ‹è¯•
            edge_gen_results = self.benchmark_edge_generation()
            
            # Edgeå¤„ç†å™¨æ€§èƒ½æµ‹è¯•
            processor_results = self.benchmark_edge_processor(batch_sizes)
            
            # å†…å­˜ä½¿ç”¨æµ‹è¯•
            memory_results = self.benchmark_memory_usage(max_size)
            
            # ä¿å­˜ç»“æœ
            self.save_results(edge_gen_results, processor_results, memory_results)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆ!")
            print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}")
            print("=" * 60)
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Edge Mapæ€§èƒ½æµ‹è¯•è„šæœ¬")
    
    parser.add_argument("--device", type=str, default="auto",
                       choices=["auto", "cuda", "cpu"],
                       help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--output_dir", type=str, default="performance_test_results",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--batch_sizes", type=str, default="1,2,4,8",
                       help="æµ‹è¯•çš„batch sizesï¼Œç”¨é€—å·åˆ†éš”")
    parser.add_argument("--max_size", type=str, default="2048,2048",
                       help="æœ€å¤§æµ‹è¯•å°ºå¯¸ï¼Œæ ¼å¼: width,height")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    # è§£æbatch sizes
    batch_sizes = [int(x.strip()) for x in args.batch_sizes.split(',')]
    
    # è§£ææœ€å¤§å°ºå¯¸
    max_size = tuple(int(x.strip()) for x in args.max_size.split(','))
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = EdgeMapPerformanceTester(device=device, output_dir=args.output_dir)
    
    # è¿è¡Œæµ‹è¯•
    success = tester.run_all_tests(batch_sizes=batch_sizes, max_size=max_size)
    
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
