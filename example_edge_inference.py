#!/usr/bin/env python3
"""
StableSR Edge Map æ¨ç†ç¤ºä¾‹
æœ€ç®€å•çš„edgeæ¨¡å‹æ¨ç†ç¤ºä¾‹ä»£ç 
"""

import torch
import cv2
import numpy as np
from PIL import Image
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from ldm.models.diffusion.ddpm_with_edge import LatentDiffusionSRTextWTWithEdge
from tra_report import EdgeDDIMSampler
import torch.nn.functional as F
from omegaconf import OmegaConf


def example_edge_inference():
    """Edgeæ¨¡å‹æ¨ç†ç¤ºä¾‹"""
    print("StableSR Edge Map æ¨ç†ç¤ºä¾‹")
    print("="*40)
    
    # 1. é…ç½®è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    config_path = "configs/stableSRNew/v2-finetune_text_T_512_edge.yaml"
    ckpt_path = "/stablesr_dataset/checkpoints/stablesr_turbo.ckpt"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("è¯·ä¿®æ”¹config_pathä¸ºæ­£ç¡®çš„é…ç½®æ–‡ä»¶è·¯å¾„")
        return
    
    if not os.path.exists(ckpt_path):
        print(f"âŒ æ¨¡å‹æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {ckpt_path}")
        print("è¯·ä¿®æ”¹ckpt_pathä¸ºæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return
    
    try:
        # 2. åŠ è½½æ¨¡å‹
        print("1. åŠ è½½æ¨¡å‹...")
        config = OmegaConf.load(config_path)
        
        # ä»é…ç½®ä¸­æå–æ¨¡å‹å‚æ•°
        model_config = config.model.params
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = LatentDiffusionSRTextWTWithEdge(
            first_stage_config=model_config.first_stage_config,
            cond_stage_config=model_config.cond_stage_config,
            structcond_stage_config=model_config.structcond_stage_config,
            num_timesteps_cond=model_config.get('num_timesteps_cond', 1),
            cond_stage_key=model_config.get('cond_stage_key', 'image'),
            cond_stage_trainable=model_config.get('cond_stage_trainable', False),
            concat_mode=model_config.get('concat_mode', True),
            conditioning_key=model_config.get('conditioning_key', 'crossattn'),
            scale_factor=model_config.get('scale_factor', 0.18215),
            scale_by_std=model_config.get('scale_by_std', False),
            unfrozen_diff=model_config.get('unfrozen_diff', False),
            random_size=model_config.get('random_size', False),
            test_gt=model_config.get('test_gt', False),
            p2_gamma=model_config.get('p2_gamma', None),
            p2_k=model_config.get('p2_k', None),
            time_replace=model_config.get('time_replace', 1000),
            use_usm=model_config.get('use_usm', True),
            mix_ratio=model_config.get('mix_ratio', 0.0),
            use_edge_processing=model_config.get('use_edge_processing', True),
            edge_input_channels=model_config.get('edge_input_channels', 3),
            linear_start=model_config.get('linear_start', 0.00085),
            linear_end=model_config.get('linear_end', 0.0120),
            timesteps=model_config.get('timesteps', 1000),
            first_stage_key=model_config.get('first_stage_key', 'image'),
            image_size=model_config.get('image_size', 512),
            channels=model_config.get('channels', 4),
            unet_config=model_config.get('unet_config', None),
            use_ema=model_config.get('use_ema', False)
        ).cuda()
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        print("åŠ è½½æ£€æŸ¥ç‚¹...")
        model.init_from_ckpt(ckpt_path)
        
        sampler = EdgeDDIMSampler(model)
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 3. åˆ›å»ºæµ‹è¯•å›¾åƒ
        print("2. åˆ›å»ºæµ‹è¯•å›¾åƒ...")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒï¼ˆåŒ…å«è¾¹ç¼˜ï¼‰
        test_img = np.ones((256, 256, 3), dtype=np.float32)
        
        # æ·»åŠ ä¸€äº›å‡ ä½•å½¢çŠ¶æ¥äº§ç”Ÿè¾¹ç¼˜
        cv2.rectangle(test_img, (50, 50), (150, 150), (0.8, 0.2, 0.2), -1)  # çº¢è‰²çŸ©å½¢
        cv2.circle(test_img, (200, 200), 50, (0.2, 0.8, 0.2), -1)           # ç»¿è‰²åœ†å½¢
        cv2.line(test_img, (100, 200), (250, 100), (0.2, 0.2, 0.8), 3)      # è“è‰²çº¿æ¡
        
        # è½¬æ¢ä¸ºtensor
        img_tensor = torch.from_numpy(test_img).permute(2, 0, 1).unsqueeze(0)
        img_tensor = (img_tensor - 0.5) / 0.5  # å½’ä¸€åŒ–åˆ°[-1, 1]
        img_tensor = img_tensor.cuda()
        print(f"âœ“ æµ‹è¯•å›¾åƒåˆ›å»ºå®Œæˆ: {img_tensor.shape}")
        
        # 4. ä¸Šé‡‡æ ·å›¾åƒ
        print("3. ä¸Šé‡‡æ ·å›¾åƒ...")
        upscaled_img = F.interpolate(img_tensor, size=(512, 512), mode='bicubic')
        print(f"âœ“ å›¾åƒä¸Šé‡‡æ ·å®Œæˆ: {upscaled_img.shape}")
        
        # 5. ç”Ÿæˆedge map
        print("4. ç”Ÿæˆedge map...")
        edge_map = generate_edge_map(upscaled_img)
        print(f"âœ“ Edge mapç”Ÿæˆå®Œæˆ: {edge_map.shape}")
        
        # 6. å‡†å¤‡æ¨ç†æ¡ä»¶
        print("5. å‡†å¤‡æ¨ç†æ¡ä»¶...")
        
        # æ–‡æœ¬æ¡ä»¶ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ— æ–‡æœ¬ï¼‰
        cross_attn = model.get_learned_conditioning([""])
        
        # ç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        encoder_posterior = model.encode_first_stage(upscaled_img)
        z_upscaled = model.get_first_stage_encoding(encoder_posterior).detach()
        
        # ç”Ÿæˆç»“æ„æ¡ä»¶
        struct_cond = model.structcond_stage_model(z_upscaled, torch.zeros(1, device='cuda'))
        
        # æ„å»ºæ¡ä»¶å­—å…¸
        conditioning = {
            "c_concat": upscaled_img,
            "c_crossattn": cross_attn,
            "struct_cond": struct_cond,
            "edge_map": edge_map
        }
        print("âœ“ æ¨ç†æ¡ä»¶å‡†å¤‡å®Œæˆ")
        
        # 7. æ‰§è¡Œæ¨ç†
        print("6. æ‰§è¡Œæ¨ç†...")
        with torch.no_grad():
            samples, _ = sampler.sample(
                S=20,  # DDPMæ­¥æ•°
                conditioning=conditioning,
                batch_size=1,
                shape=(4, 64, 64),  # æ½œåœ¨ç©ºé—´å½¢çŠ¶
                verbose=True
            )
        print("âœ“ æ¨ç†å®Œæˆ")
        
        # 8. è§£ç ç»“æœ
        print("7. è§£ç ç»“æœ...")
        result = model.decode_first_stage(samples)
        result = torch.clamp((result + 1.0) / 2.0, min=0.0, max=1.0)
        print("âœ“ ç»“æœè§£ç å®Œæˆ")
        
        # 9. ä¿å­˜ç»“æœ
        print("8. ä¿å­˜ç»“æœ...")
        os.makedirs("example_output", exist_ok=True)
        
        # ä¿å­˜è¾“å…¥å›¾åƒ
        input_np = upscaled_img[0].cpu().permute(1, 2, 0).numpy()
        input_np = (input_np + 1.0) / 2.0
        input_np = (input_np * 255).astype(np.uint8)
        Image.fromarray(input_np).save("example_output/input.png")
        
        # ä¿å­˜edge map
        edge_np = edge_map[0].cpu().permute(1, 2, 0).numpy()
        edge_np = (edge_np + 1.0) / 2.0
        edge_np = (edge_np * 255).astype(np.uint8)
        Image.fromarray(edge_np).save("example_output/edge_map.png")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        result_np = result[0].cpu().permute(1, 2, 0).numpy()
        result_np = (result_np * 255).astype(np.uint8)
        Image.fromarray(result_np).save("example_output/result.png")
        
        print("âœ“ ç»“æœä¿å­˜å®Œæˆ")
        print("\nç»“æœæ–‡ä»¶:")
        print("  - example_output/input.png: è¾“å…¥å›¾åƒ")
        print("  - example_output/edge_map.png: Edge map")
        print("  - example_output/result.png: è¶…åˆ†è¾¨ç‡ç»“æœ")
        
        print("\nğŸ‰ ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def generate_edge_map(image_tensor):
    """ç”Ÿæˆedge mapçš„ç®€åŒ–å‡½æ•°"""
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    img_np = image_tensor[0].cpu().numpy()
    img_np = (img_np + 1.0) / 2.0  # ä»[-1, 1]è½¬æ¢åˆ°[0, 1]
    img_np = np.clip(img_np, 0, 1)
    img_np = np.transpose(img_np, (1, 2, 0))  # ä»[C, H, W]è½¬æ¢åˆ°[H, W, C]
    
    # è½¬æ¢ä¸ºç°åº¦å›¾
    img_gray = cv2.cvtColor((img_np * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
    
    # åº”ç”¨é«˜æ–¯æ¨¡ç³Š
    img_blurred = cv2.GaussianBlur(img_gray, (5, 5), 1.4)
    
    # åº”ç”¨Cannyè¾¹ç¼˜æ£€æµ‹
    edges = cv2.Canny(img_blurred, threshold1=100, threshold2=200)
    
    # è½¬æ¢ä¸º3é€šé“
    edges_3ch = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
    
    # è½¬æ¢å›tensoræ ¼å¼
    edges_tensor = torch.from_numpy(edges_3ch).permute(2, 0, 1).unsqueeze(0).float()
    edges_tensor = (edges_tensor / 127.5) - 1.0  # å½’ä¸€åŒ–åˆ°[-1, 1]
    
    return edges_tensor.to(image_tensor.device)


if __name__ == "__main__":
    example_edge_inference()
